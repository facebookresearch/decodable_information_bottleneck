{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimax Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import typing\n",
    "\n",
    "import higher\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim=2):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(784, 1024), nn.ReLU(inplace=True), nn.Linear(1024, z_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.module(torch.flatten(x, start_dim=1))\n",
    "\n",
    "\n",
    "class Q(nn.Module):\n",
    "    def __init__(self, z_dim=2, n_cls: int = 10):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(z_dim, 4048), nn.ReLU(inplace=True), nn.Linear(4048, n_cls),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.module(z)\n",
    "\n",
    "\n",
    "class Qminus(nn.Module):\n",
    "    def __init__(self, z_dim=2, n_cls: int = 10):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(nn.Linear(z_dim, n_cls))\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.module(z)\n",
    "\n",
    "\n",
    "class DIB(nn.Module):\n",
    "   \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_small_adv: bool = True,\n",
    "        n_cls: int = 10,\n",
    "        n_inner_iter: int = 5,\n",
    "        is_adv: bool = True,\n",
    "        beta: int = 1,\n",
    "        gamma: int = 1,\n",
    "        is_higher: bool = True,\n",
    "        is_load=True,\n",
    "        z_dim=2,\n",
    "        is_single_opt=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if is_small_adv:\n",
    "            self.q, self.q_adv = (\n",
    "                Q(z_dim=z_dim, n_cls=n_cls),\n",
    "                Qminus(z_dim=z_dim, n_cls=n_cls),\n",
    "            )\n",
    "        else:\n",
    "            self.q_adv, self.q = (\n",
    "                Q(z_dim=z_dim, n_cls=n_cls),\n",
    "                Qminus(z_dim=z_dim, n_cls=n_cls),\n",
    "            )\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self._n_inner_iter = n_inner_iter\n",
    "        self.is_adv = is_adv\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.is_higher = is_higher\n",
    "        self.is_load = is_load\n",
    "        self.is_single_opt = is_single_opt\n",
    "        \n",
    "        if self.is_single_opt:\n",
    "            self.inner_opt = torch.optim.Adam(self.q_adv.parameters(), lr=1e-3)\n",
    "\n",
    "    @property\n",
    "    def n_inner_iter(self):\n",
    "        return self._n_inner_iter if self.training else 1000\n",
    "\n",
    "    def forward(self, z, y):\n",
    "\n",
    "        loss_clf = F.cross_entropy(self.q(z), y)\n",
    "\n",
    "        if not self.training:\n",
    "            q_adv_old = self.q_adv\n",
    "            self.q_adv = copy.deepcopy(self.q_adv)\n",
    "\n",
    "        if self.is_higher and self.training:\n",
    "            self.train_heads_higher(z, y)\n",
    "        else:\n",
    "            self.train_heads_freeze(z, y)\n",
    "\n",
    "        pred_adv = self.q_adv(z)\n",
    "        loss_enc = F.cross_entropy(pred_adv, y)\n",
    "\n",
    "        if not self.training:\n",
    "            self.q_adv = q_adv_old\n",
    "\n",
    "        if self.is_adv:\n",
    "            loss_enc = -loss_enc\n",
    "\n",
    "        return self.beta * loss_clf, self.gamma * loss_enc, self.q(z), pred_adv\n",
    "\n",
    "    def train_heads_higher_old(self, z, y):\n",
    "\n",
    "        # Define a differentiable optimizer to update the label with.\n",
    "        inner_opt = higher.get_diff_optim(\n",
    "            torch.optim.Adam(self.q_adv.parameters(), lr=1e-3), self.q_adv.parameters()\n",
    "        )\n",
    "\n",
    "        # Take a few gradient steps to find good heads\n",
    "        for _ in range(self.n_inner_iter):\n",
    "            loss_adv = F.cross_entropy(self.q_adv(z), y)\n",
    "            inner_opt.step(loss_adv, params=self.q_adv.parameters())\n",
    "\n",
    "    def train_heads_higher(self, z, y):\n",
    "\n",
    "        inner_opt = torch.optim.Adam(self.q_adv.parameters(), lr=1e-3)\n",
    "        with higher.innerloop_ctx(\n",
    "            self.q_adv,\n",
    "            inner_opt,\n",
    "            copy_initial_weights=False,\n",
    "            track_higher_grads=self.training,\n",
    "        ) as (fnet, diffopt):\n",
    "\n",
    "            # Take a few gradient steps to find good heads\n",
    "            for _ in range(self.n_inner_iter):\n",
    "                loss_adv = F.cross_entropy(fnet(z), y)\n",
    "                diffopt.step(loss_adv)\n",
    "\n",
    "            if self.is_load:\n",
    "                self.q_adv.load_state_dict(fnet.state_dict())\n",
    "\n",
    "    def train_heads_freeze(self, z, y):\n",
    "\n",
    "        z = z.detach()  # don't backprop through encoder\n",
    "        inner_opt = torch.optim.Adam(self.q_adv.parameters(), lr=1e-3)\n",
    "\n",
    "        def closure():\n",
    "            inner_opt.zero_grad()\n",
    "            loss_adv = F.cross_entropy(self.q_adv(z), y)\n",
    "            loss_adv.backward()\n",
    "            return loss_adv\n",
    "\n",
    "        for i in range(self.n_inner_iter):\n",
    "            inner_opt.step(closure)\n",
    "\n",
    "\n",
    "class ERM(nn.Module):\n",
    "    def __init__(\n",
    "        self, *args, n_cls: int = 10, z_dim=2, **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.q = Qminus(n_cls)\n",
    "        self.q_adv = self.q\n",
    "\n",
    "    def forward(self, z, y):\n",
    "\n",
    "        loss_clf = F.cross_entropy(self.q(z), y)\n",
    "\n",
    "        return loss_clf, 0 * loss_clf\n",
    "\n",
    "\n",
    "def train(model, criterion, device, train_loader, optimizer, epoch, log_interval=20):\n",
    "\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z = model(data)\n",
    "        loss_clf, loss_enc, _, _ = criterion(z, target)\n",
    "        (loss_clf + loss_enc).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if log_interval != 0 and batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss Q: {:.6f}\\tLoss Q adv: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss_clf.item(),\n",
    "                    loss_enc.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model, criterion, device, test_loader, epoch):\n",
    "    \"\"\"Evaluate the performance on the test dataset.\"\"\"\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "    test_loss_adv = 0\n",
    "    test_loss_q = 0\n",
    "    correct_q = 0\n",
    "    correct_adv = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        z = model(data)\n",
    "        loss_clf, loss_enc, pred_q, pred_adv = criterion(z, target)\n",
    "        test_loss_adv -= loss_enc.sum().item()\n",
    "        test_loss_q += loss_clf.sum().item()\n",
    "        pred_q = pred_q.argmax(dim=1, keepdim=True)\n",
    "        pred_adv = pred_adv.argmax(dim=1, keepdim=True)\n",
    "        correct_q += pred_q.eq(target.view_as(pred_q)).sum().item()\n",
    "        correct_adv += pred_adv.eq(target.view_as(pred_adv)).sum().item()\n",
    "\n",
    "    test_loss_adv /= len(test_loader.dataset)\n",
    "    test_loss_q /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTesting in Epoch {}: Average loss: q={:.8f}; adv={:.8f}  \\t Accuracy Q : {:.2f}% \\t Accuracy Adv : {:.2f}%\\n\".format(\n",
    "            epoch,\n",
    "            test_loss_q,\n",
    "            test_loss_adv,\n",
    "            100.0 * correct_q / len(test_loader.dataset),\n",
    "            100.0 * correct_adv / len(test_loader.dataset),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "n_train = 5000\n",
    "n_test = 1000\n",
    "idcs_train = list(range(n_train))\n",
    "np.random.shuffle(idcs_train)\n",
    "\n",
    "\n",
    "def get_train_data_loader(batch_size):\n",
    "    return DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"/tmp/mnist-data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "\n",
    "train_loader = get_train_data_loader(256)\n",
    "test_loader = get_train_data_loader(4096)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `is_load`\n",
    "Effect of loading the inner module vs restarting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_load: True\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00013010; adv=0.00011968  \t Accuracy Q : 86.58% \t Accuracy Adv : 85.07%\n",
      "\n",
      "is_load: False\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.66131139; adv=0.03873382  \t Accuracy Q : 15.49% \t Accuracy Adv : 2.06%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for is_load in [True, False]:\n",
    "    print(\"is_load:\", is_load)\n",
    "\n",
    "    z_dim = 10\n",
    "    encoder = Encoder(z_dim=z_dim).to(device)\n",
    "    criterion = DIB(n_inner_iter=10, is_higher=True, z_dim=z_dim, is_load=is_load).to(\n",
    "        device\n",
    "    )\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(criterion.q.parameters()), lr=3e-4\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train(\n",
    "            encoder, criterion, device, train_loader, optimizer, epoch, log_interval=0\n",
    "        )\n",
    "\n",
    "    test(encoder, criterion, device, test_loader, epoch)\n",
    "\n",
    "    encoder.to(torch.device(\"cpu\"))\n",
    "    criterion.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `z_dim`\n",
    "Effect of dimensionality of z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_dim: 2\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00021608; adv=0.00054495  \t Accuracy Q : 73.64% \t Accuracy Adv : 30.67%\n",
      "\n",
      "z_dim: 4\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00078564; adv=0.00013315  \t Accuracy Q : 77.24% \t Accuracy Adv : 84.41%\n",
      "\n",
      "z_dim: 8\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00009704; adv=0.00046859  \t Accuracy Q : 87.64% \t Accuracy Adv : 35.44%\n",
      "\n",
      "z_dim: 128\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00008146; adv=0.00005935  \t Accuracy Q : 89.39% \t Accuracy Adv : 93.23%\n",
      "\n",
      "z_dim: 1024\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00069846; adv=0.00112772  \t Accuracy Q : 54.63% \t Accuracy Adv : 83.44%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for z_dim in [2, 4, 8, 128, 1024]:\n",
    "    print(\"z_dim:\", z_dim)\n",
    "\n",
    "    encoder = Encoder(z_dim=z_dim).to(device)\n",
    "    criterion = DIB(n_inner_iter=10, is_higher=True, z_dim=z_dim, is_load=True).to(\n",
    "        device\n",
    "    )\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(criterion.q.parameters()), lr=3e-4\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train(\n",
    "            encoder, criterion, device, train_loader, optimizer, epoch, log_interval=0\n",
    "        )\n",
    "\n",
    "    test(encoder, criterion, device, test_loader, epoch)\n",
    "\n",
    "    encoder.to(torch.device(\"cpu\"))\n",
    "    criterion.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `is_higher`\n",
    "backprop in optimization or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_higher: True\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00009625; adv=0.00043195  \t Accuracy Q : 88.99% \t Accuracy Adv : 43.88%\n",
      "\n",
      "is_higher: False\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00015368; adv=0.00037276  \t Accuracy Q : 82.59% \t Accuracy Adv : 49.93%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for is_higher in [True, False]:\n",
    "    print(\"is_higher:\", is_higher)\n",
    "\n",
    "    z_dim = 10\n",
    "    encoder = Encoder(z_dim=z_dim).to(device)\n",
    "    criterion = DIB(is_adv=True, n_inner_iter=10, is_higher=is_higher, z_dim=z_dim).to(\n",
    "        device\n",
    "    )\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(criterion.q.parameters()), lr=3e-4\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train(\n",
    "            encoder, criterion, device, train_loader, optimizer, epoch, log_interval=0\n",
    "        )\n",
    "\n",
    "    test(encoder, criterion, device, test_loader, epoch)\n",
    "\n",
    "    encoder.to(torch.device(\"cpu\"))\n",
    "    criterion.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `n_inner_iter`\n",
    "Effect of inner optimization steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_inner_iter: 2\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00013499; adv=0.00009040  \t Accuracy Q : 90.72% \t Accuracy Adv : 89.28%\n",
      "\n",
      "n_inner_iter: 10\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00029147; adv=0.00022898  \t Accuracy Q : 77.12% \t Accuracy Adv : 70.35%\n",
      "\n",
      "n_inner_iter: 100\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00002769; adv=0.00055824  \t Accuracy Q : 96.93% \t Accuracy Adv : 26.43%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_inner_iter in [2, 10, 100]:\n",
    "    print(\"n_inner_iter:\", n_inner_iter)\n",
    "\n",
    "    z_dim = 10\n",
    "    encoder = Encoder(z_dim=z_dim).to(device)\n",
    "    criterion = DIB(\n",
    "        n_inner_iter=n_inner_iter, is_higher=True, z_dim=z_dim, is_load=True\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(criterion.q.parameters()), lr=3e-4\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train(\n",
    "            encoder, criterion, device, train_loader, optimizer, epoch, log_interval=0\n",
    "        )\n",
    "\n",
    "    test(encoder, criterion, device, test_loader, epoch)\n",
    "\n",
    "    encoder.to(torch.device(\"cpu\"))\n",
    "    criterion.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `batch_size`\n",
    "Effect of batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**on train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00007423; adv=0.00051259  \t Accuracy Q : 92.00% \t Accuracy Adv : 27.89%\n",
      "\n",
      "batch_size: 256\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00002989; adv=0.00055338  \t Accuracy Q : 96.61% \t Accuracy Adv : 24.83%\n",
      "\n",
      "batch_size: 1024\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00010344; adv=0.00053473  \t Accuracy Q : 88.70% \t Accuracy Adv : 28.86%\n",
      "\n",
      "batch_size: 4096\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00017944; adv=0.00017987  \t Accuracy Q : 76.36% \t Accuracy Adv : 76.94%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [64,256,1024,4096]:\n",
    "    print(\"batch_size:\", batch_size)\n",
    "\n",
    "    z_dim = 10\n",
    "    encoder = Encoder(z_dim=z_dim).to(device)\n",
    "    criterion = DIB(\n",
    "        n_inner_iter=100, is_higher=True, z_dim=z_dim, is_load=True\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(criterion.q.parameters()), lr=3e-4\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train(\n",
    "            encoder, criterion, device, get_train_data_loader(batch_size), optimizer, epoch, log_interval=0\n",
    "        )\n",
    "\n",
    "    test(encoder, criterion, device, test_loader, epoch)\n",
    "\n",
    "    encoder.to(torch.device(\"cpu\"))\n",
    "    criterion.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**on test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.01127896; adv=0.00428821  \t Accuracy Q : 76.36% \t Accuracy Adv : 94.06%\n",
      "\n",
      "batch_size: 256\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00282493; adv=0.00210018  \t Accuracy Q : 76.36% \t Accuracy Adv : 82.94%\n",
      "\n",
      "batch_size: 1024\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00070806; adv=0.00066659  \t Accuracy Q : 76.36% \t Accuracy Adv : 78.20%\n",
      "\n",
      "batch_size: 4096\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00017944; adv=0.00017987  \t Accuracy Q : 76.36% \t Accuracy Adv : 76.94%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for batch_size in [64,256,1024,4096]:\n",
    "    print(\"batch_size:\", batch_size)\n",
    "    test(encoder, criterion, device, get_train_data_loader(batch_size), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00022767; adv=0.00036803  \t Accuracy Q : 81.67% \t Accuracy Adv : 50.72%\n",
      "\n",
      "batch_size: 256\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00027189; adv=0.00020321  \t Accuracy Q : 76.57% \t Accuracy Adv : 73.23%\n",
      "\n",
      "batch_size: 1024\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00021287; adv=0.00021211  \t Accuracy Q : 75.14% \t Accuracy Adv : 77.05%\n",
      "\n",
      "batch_size: 4096\n",
      "\n",
      "Testing in Epoch 4: Average loss: q=0.00012790; adv=0.00012246  \t Accuracy Q : 86.91% \t Accuracy Adv : 87.30%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [64,256,1024,4096]:\n",
    "    print(\"batch_size:\", batch_size)\n",
    "\n",
    "    z_dim = 10\n",
    "    encoder = Encoder(z_dim=z_dim).to(device)\n",
    "    criterion = DIB(\n",
    "        n_inner_iter=10, is_higher=True, z_dim=z_dim, is_load=True\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(criterion.q.parameters()), lr=3e-4\n",
    "    )\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train(\n",
    "            encoder, criterion, device, get_train_data_loader(batch_size), optimizer, epoch, log_interval=0\n",
    "        )\n",
    "\n",
    "    test(encoder, criterion, device, test_loader, epoch)\n",
    "\n",
    "    encoder.to(torch.device(\"cpu\"))\n",
    "    criterion.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=torch.rand(2,3,4,5,6,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.view(*t.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(5)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogCumsumExp(function.Function):\n",
    "\n",
    "    def check_type_forward(self, in_types):\n",
    "        type_check.expect(\n",
    "            in_types.size() == 1,\n",
    "            in_types[0].dtype.kind == 'f',\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        xp = cuda.get_array_module(*inputs)\n",
    "\n",
    "        x, = inputs\n",
    "        m = x.max(axis=0, keepdims=True)\n",
    "        y = x - m\n",
    "        xp.exp(y, out=y)\n",
    "        y_sum = xp.flip(xp.cumsum(xp.flip(y, axis=0)), axis=0)\n",
    "        self.y = xp.transpose(xp.asarray(xp.log(y_sum) + m))\n",
    "        return self.y,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_cumsum_exp(x, dim):\n",
    "    m = x.max(axis=0, keepdims=True)[0]\n",
    "    y = torch.exp(x - m)\n",
    "    y_sum = torch.flip(torch.cumsum(torch.flip(y, 0)), 0)\n",
    "    return torch.log(y_sum) + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logcumsumexp(x, dim):\n",
    "    # slow implementation, but ok for now\n",
    "    if (dim != -1) or (dim != x.ndimension() - 1):\n",
    "        x = x.transpose(dim, -1)\n",
    "\n",
    "    out = []\n",
    "    for i in range(1, x.size(-1) + 1):\n",
    "        out.append(torch.logsumexp(x[..., :i], dim=-1, keepdim=True))\n",
    "    out = torch.cat(out, dim=-1)\n",
    "\n",
    "    if (dim != -1) or (dim != x.ndimension() - 1):\n",
    "        out = out.transpose(-1, dim)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logcumsumexp(x, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.rand(5,3) - torch.rand(5,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(t,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3446)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logcumsumexp(t.log(),1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cummax(x, dim):\n",
    "    x_np = x.detach().cpu().numpy()\n",
    "    ret = np.maximum.accumulate(x_np, axis=dim)\n",
    "    return torch.from_numpy(ret).to(x)\n",
    "\n",
    "\n",
    "def logcumsumexp(x, dim=-1):\n",
    "    if (dim != -1) or (dim != x.ndimension() - 1):\n",
    "        x = x.transpose(dim, -1)\n",
    "\n",
    "    init_size = x.size()\n",
    "    last_dim_size = init_size[-1]\n",
    "    x_resized = x.contiguous().view(-1, last_dim_size)\n",
    "    d1, d2 = x_resized.size()\n",
    "    x_cummax = cummax(x_resized, -1).view(d1, d2, 1)\n",
    "    x_expand = x_resized.unsqueeze(1).expand(d1, d2, last_dim_size)\n",
    "    mask = torch.tril(torch.ones(last_dim_size, last_dim_size)).unsqueeze(0)\n",
    "    ret = torch.log(torch.sum(torch.exp(x_expand - x_cummax) * mask, dim=-1)) + x_cummax.view(d1, d2)\n",
    "    ret = ret.view(*init_size)\n",
    "\n",
    "    if (dim != -1) or (dim != x.ndimension() - 1):\n",
    "        ret = ret.transpose(-1, dim)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3446)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([0,1,2,3])\n",
    "a[np.array(a%2 == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_discrete\n",
    "\n",
    "class LightTailPareto(rv_discrete):\n",
    "    def _cdf(self, k, alpha):\n",
    "        # alpha is factor like in SUMO paper\n",
    "        # m is minimum number of samples\n",
    "        m = self.a # lower bound of support\n",
    "        \n",
    "        # in the paper they us P(K >= k) but cdf is P(K <= k) = 1 - P(K > k) = 1 - P(K >= k + 1)\n",
    "        k = k + 1\n",
    "        \n",
    "        # make sure has at least m samples\n",
    "        k = np.clip(k - m, a_min=1, a_max=None) # makes sure no division by 0\n",
    "        alpha = alpha - m\n",
    "        \n",
    "        # sample using pmf 1/k but with finite expectation\n",
    "        cdf = 1 - np.where(k < alpha, 1/k, (1/alpha) * (0.9)**(k-alpha))\n",
    "        \n",
    "        return cdf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = LightTailPareto(a=3).freeze(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "xk = np.arange(20)\n",
    "ax.plot(xk, dist.cdf(xk), 'ro', ms=12, mec='r')\n",
    "ax.vlines(xk, 0, dist.cdf(xk), colors='r', lw=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19999999999999996"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-dist.cdf(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.support()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scipy.stats._distn_infrastructure.rv_frozen at 0x7f60a4748950>"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - dist.cdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/yannd/projects/Decodable_Information_Bottleneck\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from utils.data.imgs import get_Dataset\n",
    "from utils.data.helpers import get_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10 = get_Dataset(\"cifar10\")(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.4914009 , 0.48215896, 0.4465308 ], dtype=float32),\n",
       " array([0.24703279, 0.24348423, 0.26158753], dtype=float32))"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mean_std(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mclf_nhid_128\u001b[0m/  \u001b[01;34mtransformer\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../tmp_results/corrResnets/data_svhn/datasize_all/augment_False/rand_False/schedule_decay/optim_adam/lr_5e-05/chckpnt_last/wdecay_0/model_erm/dropout_0/encoder_resnet34/enc_zy_nhid_128/enc_zy_nlay_2/enc_zy_kpru_0/enc_zx_nhid_128/enc_zx_nlay_2/enc_zx_kpru_0/beta_1/nskip_0/zdim_1024/minimax_0/mchead_3/clfs_default/run_0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(\"../tmp_results/corrResnets/**/clf_nhid*/\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for f in files:\n",
    "    shutil.rmtree(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_permuter(n_idcs, seed=123):\n",
    "    \"\"\"Return permuted indices.\n",
    "    \n",
    "    Paramaters\n",
    "    ----------\n",
    "    n_idcs : int or array-like of int\n",
    "        Number of indices. If list, it should be a partion of the real number of idcs.\n",
    "        Each partition will be permuted separately.\n",
    "        \n",
    "    seed : int, optional\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(n_idcs, int):\n",
    "        idcs = list(range(n_idcs))\n",
    "    else:\n",
    "        idcs = [list(range(partition)) for partition in n_idcs]\n",
    "\n",
    "    with tmp_seed(seed):\n",
    "        if isinstance(n_idcs, int):\n",
    "            random.shuffle(idcs)\n",
    "            idcs = torch.tensor(idcs)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # shuffle each partition separetly\n",
    "            for partition_idcs in idcs:\n",
    "                random.shuffle(partition_idcs)\n",
    "\n",
    "            idcs = torch.cat([torch.tensor(idcs) for idcs in idcs])\n",
    "\n",
    "    return idcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_hyperparam.py  LICENSE         \u001b[0m\u001b[01;34m__pycache__\u001b[0m/        \u001b[01;34mresults_old\u001b[0m/\n",
      "aggregate.py       load_models.py  README.md           \u001b[01;34mtensorboard\u001b[0m/\n",
      "\u001b[01;34mbin\u001b[0m/               main.py         requirements_fb.sh  \u001b[01;34mtmp_results\u001b[0m/\n",
      "\u001b[01;34mconf\u001b[0m/              \u001b[01;34mnotebooks\u001b[0m/      requirements.txt    \u001b[01;34mutils\u001b[0m/\n",
      "\u001b[01;34mdib\u001b[0m/               \u001b[01;34moutputs\u001b[0m/        \u001b[01;34mresults\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dib.utils.helpers import get_idx_permuter\n",
    "_rand_indcs = get_idx_permuter([3,6,9], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 1, 4, 5, 2, 3, 0, 7, 8, 3, 4, 1, 0, 2, 5, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rand_indcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
